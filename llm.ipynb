{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import os\n",
    "import requests\n",
    "import tiktoken\n",
    "import pandas as pd\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_length= 16\n",
    "d_model = 64\n",
    "batch_size = 4\n",
    "num_heads = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('dataset.txt', 'r') as f:\n",
    "    text = f.read()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoding = tiktoken.get_encoding('cl100k_base')\n",
    "tokenized_text = encoding.encode(text)\n",
    "tokenized_text = torch.tensor(tokenized_text, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_index = int(len(tokenized_text) * 0.9)\n",
    "train_data = tokenized_text[:train_index]\n",
    "validation_data = tokenized_text[train_index:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = train_data\n",
    "indexs = torch.randint(low=0, high=len(data) - token_length, size=(batch_size,))\n",
    "# 4 * 16 tensor\n",
    "x_batch = torch.stack([data[idx: idx + token_length] for idx in indexs])\n",
    "y_batch = torch.stack([data[idx + 1: idx + token_length + 1] for idx in indexs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'?\"\\nActive listening skills complement effective questioning techniques. By actively listening to your customers,'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pd.DataFrame(x_batch[0].numpy())\n",
    "encoding.decode(x_batch[0].numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100069"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get max index of token\n",
    "max_token_value = tokenized_text.max().item()\n",
    "max_token_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.8140, -0.4648,  1.0317,  ..., -0.3554,  0.8825, -1.4436],\n",
       "        [-1.7957, -0.6973,  0.4765,  ..., -0.0712,  0.7564, -0.6118],\n",
       "        [ 1.4702, -0.9546,  1.4693,  ..., -0.1258,  0.0822, -0.2688],\n",
       "        ...,\n",
       "        [ 0.4427,  0.1723, -0.6616,  ..., -0.7277,  0.0448, -0.4593],\n",
       "        [ 1.2491, -1.9159, -0.4042,  ...,  1.6459,  0.8562, -0.3897],\n",
       "        [-0.7713, -0.1298,  0.5199,  ...,  0.6477,  1.8090,  0.1579]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# construct a 100070 * 16 matrix, get input embedding\n",
    "input_embedding_table = nn.Embedding(max_token_value + 1, d_model)\n",
    "input_embedding_table.weight.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 16, 64])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_batch_embedding = input_embedding_table(x_batch)\n",
    "y_batch_embedding = input_embedding_table(y_batch)\n",
    "x_batch_embedding.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 16, 64])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get postional encoding\n",
    "# first 16 * 16 matrix\n",
    "postion_encoding_table = torch.zeros(token_length, d_model)\n",
    "# postion 16 * 1\n",
    "postion = torch.arange(0, token_length, dtype=torch.float).unsqueeze(1)\n",
    "div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "# [:, 0::2], ':' = selecting all lines, '0::2' = starting from 0, step by step 2\n",
    "postion_encoding_table[:, 0::2] = torch.sin(postion * div_term)\n",
    "postion_encoding_table[:, 1::2] = torch.sin(postion * div_term)\n",
    "postion_encoding_table = postion_encoding_table.unsqueeze(0).expand(batch_size, -1, -1)\n",
    "\n",
    "postion_encoding_table.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 16, 64])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# add postion embedding with input embedding\n",
    "x = x_batch_embedding + postion_encoding_table\n",
    "y = y_batch_embedding + postion_encoding_table\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 16, 64])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Wq = nn.Linear(d_model, d_model)\n",
    "Wk = nn.Linear(d_model, d_model)\n",
    "Wv = nn.Linear(d_model, d_model)\n",
    "# linear last two dimension do the matrix mul\n",
    "Q = Wq(x)\n",
    "K = Wk(x)\n",
    "V = Wv(x)\n",
    "\n",
    "Q.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 4, 16, 16])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# [4, 16, 4, 16] -> [4, 4, 16, 16]\n",
    "# why permute? To enable each head to do separate computing which is parallel computing\n",
    "# 说人话，就是考虑的是token_length与每个head维度之间的关系，而不是num_heads与别的关系；矩阵相乘是\n",
    "Q = Q.reshape(batch_size, token_length, num_heads, d_model//num_heads).permute(0, 2, 1, 3)\n",
    "K = K.reshape(batch_size, token_length, num_heads, d_model//num_heads).permute(0, 2, 1, 3)\n",
    "V = V.reshape(batch_size, token_length, num_heads, d_model//num_heads).permute(0, 2, 1, 3)\n",
    "Q.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply the attension fomular\n",
    "output = Q @ K.transpose(-2, -1) / math.sqrt(d_model // num_heads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 0.2623,    -inf,    -inf,  ...,    -inf,    -inf,    -inf],\n",
       "          [-0.3034,  0.1390,    -inf,  ...,    -inf,    -inf,    -inf],\n",
       "          [ 0.2898,  0.1144,  0.3308,  ...,    -inf,    -inf,    -inf],\n",
       "          ...,\n",
       "          [ 0.0499, -0.3026, -0.1784,  ..., -0.5264,    -inf,    -inf],\n",
       "          [-0.4222,  0.2738,  0.2808,  ...,  1.3248, -0.3370,    -inf],\n",
       "          [-0.4129,  0.3758, -0.9122,  ...,  0.2867,  0.3695, -0.3973]],\n",
       "\n",
       "         [[-0.6873,    -inf,    -inf,  ...,    -inf,    -inf,    -inf],\n",
       "          [ 0.4880, -0.9297,    -inf,  ...,    -inf,    -inf,    -inf],\n",
       "          [ 0.2570, -0.1666,  0.0134,  ...,    -inf,    -inf,    -inf],\n",
       "          ...,\n",
       "          [-0.0403, -0.5665,  0.2625,  ...,  0.0392,    -inf,    -inf],\n",
       "          [-0.3586,  0.3388, -0.4874,  ...,  0.0802, -0.3259,    -inf],\n",
       "          [-0.3843, -0.6643,  0.3547,  ..., -0.0416, -0.7000, -0.1762]],\n",
       "\n",
       "         [[ 0.4576,    -inf,    -inf,  ...,    -inf,    -inf,    -inf],\n",
       "          [-0.0694,  0.8065,    -inf,  ...,    -inf,    -inf,    -inf],\n",
       "          [ 0.3031, -0.2841,  0.5825,  ...,    -inf,    -inf,    -inf],\n",
       "          ...,\n",
       "          [-0.1790, -0.3783,  0.1877,  ...,  0.5610,    -inf,    -inf],\n",
       "          [ 0.0493,  0.0607,  0.1887,  ..., -0.1151, -0.5220,    -inf],\n",
       "          [-0.2794, -0.0486,  0.2467,  ..., -0.2422, -0.1893, -0.0128]],\n",
       "\n",
       "         [[ 0.3267,    -inf,    -inf,  ...,    -inf,    -inf,    -inf],\n",
       "          [-0.1912,  0.0406,    -inf,  ...,    -inf,    -inf,    -inf],\n",
       "          [-0.3229,  0.0999,  0.8629,  ...,    -inf,    -inf,    -inf],\n",
       "          ...,\n",
       "          [ 0.6031, -0.1186, -0.4946,  ...,  0.0372,    -inf,    -inf],\n",
       "          [ 0.2842, -0.0582, -0.1308,  ..., -0.2709, -0.5499,    -inf],\n",
       "          [ 0.0362,  0.7147, -0.0279,  ...,  0.0936,  0.4251, -0.2402]]],\n",
       "\n",
       "\n",
       "        [[[-0.0815,    -inf,    -inf,  ...,    -inf,    -inf,    -inf],\n",
       "          [-0.2749, -0.1798,    -inf,  ...,    -inf,    -inf,    -inf],\n",
       "          [-0.4298, -0.8268, -0.2977,  ...,    -inf,    -inf,    -inf],\n",
       "          ...,\n",
       "          [ 0.3437,  0.1653, -0.6558,  ...,  0.0772,    -inf,    -inf],\n",
       "          [-0.2350,  0.4653, -0.2572,  ..., -0.5974, -0.0291,    -inf],\n",
       "          [-0.0587,  0.3549, -0.1322,  ..., -0.1662, -0.2364, -0.3698]],\n",
       "\n",
       "         [[-0.2112,    -inf,    -inf,  ...,    -inf,    -inf,    -inf],\n",
       "          [-1.0327, -0.1609,    -inf,  ...,    -inf,    -inf,    -inf],\n",
       "          [ 0.2326,  0.5878, -0.0401,  ...,    -inf,    -inf,    -inf],\n",
       "          ...,\n",
       "          [ 0.1608, -0.8088, -0.2175,  ..., -0.4417,    -inf,    -inf],\n",
       "          [ 0.2377,  0.1857,  0.1885,  ...,  0.4091,  0.3590,    -inf],\n",
       "          [-0.4216, -0.0121,  0.1610,  ...,  0.6507,  0.2352, -0.1557]],\n",
       "\n",
       "         [[ 0.1456,    -inf,    -inf,  ...,    -inf,    -inf,    -inf],\n",
       "          [-0.0601, -0.6139,    -inf,  ...,    -inf,    -inf,    -inf],\n",
       "          [ 0.0399,  0.1726,  0.2455,  ...,    -inf,    -inf,    -inf],\n",
       "          ...,\n",
       "          [ 0.0157,  0.5718, -0.0249,  ..., -0.0578,    -inf,    -inf],\n",
       "          [-0.6291,  1.0512,  0.2611,  ..., -0.4535,  0.1341,    -inf],\n",
       "          [ 0.1979,  0.6699,  0.6200,  ..., -0.0036, -0.3239,  0.1339]],\n",
       "\n",
       "         [[ 0.6149,    -inf,    -inf,  ...,    -inf,    -inf,    -inf],\n",
       "          [ 0.0135, -0.5208,    -inf,  ...,    -inf,    -inf,    -inf],\n",
       "          [-0.5864,  0.2243,  0.5401,  ...,    -inf,    -inf,    -inf],\n",
       "          ...,\n",
       "          [-0.4902, -0.0047,  0.7238,  ..., -0.3172,    -inf,    -inf],\n",
       "          [ 0.2744,  0.0492,  0.0996,  ..., -0.0664,  0.0913,    -inf],\n",
       "          [ 0.4008, -0.2287,  0.1767,  ...,  0.0759, -0.0116,  0.4176]]],\n",
       "\n",
       "\n",
       "        [[[-0.0932,    -inf,    -inf,  ...,    -inf,    -inf,    -inf],\n",
       "          [ 0.2426,  0.2117,    -inf,  ...,    -inf,    -inf,    -inf],\n",
       "          [ 0.1913,  0.1212,  0.1520,  ...,    -inf,    -inf,    -inf],\n",
       "          ...,\n",
       "          [-0.1408, -0.0418,  0.0108,  ..., -0.5088,    -inf,    -inf],\n",
       "          [ 0.0044,  0.2888, -0.1177,  ..., -0.7206,  1.2048,    -inf],\n",
       "          [ 0.1650, -0.1242,  0.1947,  ..., -0.2041, -0.7422, -0.0336]],\n",
       "\n",
       "         [[ 0.3241,    -inf,    -inf,  ...,    -inf,    -inf,    -inf],\n",
       "          [ 0.0394, -0.8681,    -inf,  ...,    -inf,    -inf,    -inf],\n",
       "          [ 0.1129, -0.2850, -0.0235,  ...,    -inf,    -inf,    -inf],\n",
       "          ...,\n",
       "          [ 0.0452, -0.0344, -0.4920,  ..., -0.2808,    -inf,    -inf],\n",
       "          [ 1.0217,  0.0303, -0.2880,  ...,  0.5171,  0.5423,    -inf],\n",
       "          [ 0.4243, -0.1809,  0.1004,  ...,  0.9341,  0.6471, -0.1846]],\n",
       "\n",
       "         [[-0.4248,    -inf,    -inf,  ...,    -inf,    -inf,    -inf],\n",
       "          [ 0.4276,  0.7301,    -inf,  ...,    -inf,    -inf,    -inf],\n",
       "          [ 0.3336,  0.4099, -0.3359,  ...,    -inf,    -inf,    -inf],\n",
       "          ...,\n",
       "          [-0.3313,  0.2176, -0.3272,  ...,  0.0534,    -inf,    -inf],\n",
       "          [-0.6775,  0.8217,  0.0152,  ...,  0.4802,  0.6377,    -inf],\n",
       "          [-0.4954,  0.6427, -0.1364,  ..., -0.3944,  0.8946, -0.5928]],\n",
       "\n",
       "         [[ 0.6871,    -inf,    -inf,  ...,    -inf,    -inf,    -inf],\n",
       "          [ 0.2959,  0.2834,    -inf,  ...,    -inf,    -inf,    -inf],\n",
       "          [ 0.1267, -0.7705,  0.1874,  ...,    -inf,    -inf,    -inf],\n",
       "          ...,\n",
       "          [ 0.1912,  0.7344,  0.4085,  ..., -0.2984,    -inf,    -inf],\n",
       "          [ 0.0063, -0.2339,  0.5020,  ...,  0.7077,  0.3950,    -inf],\n",
       "          [ 0.2498, -0.3667, -0.3003,  ..., -0.0975, -0.5492, -0.7204]]],\n",
       "\n",
       "\n",
       "        [[[-0.1889,    -inf,    -inf,  ...,    -inf,    -inf,    -inf],\n",
       "          [ 0.0278, -0.3812,    -inf,  ...,    -inf,    -inf,    -inf],\n",
       "          [ 0.3293, -0.0743,  0.4645,  ...,    -inf,    -inf,    -inf],\n",
       "          ...,\n",
       "          [ 0.0545, -0.5158, -0.1414,  ..., -0.5887,    -inf,    -inf],\n",
       "          [-0.2282, -0.9421,  0.2991,  ..., -0.6779, -1.2470,    -inf],\n",
       "          [ 0.0028,  0.1207, -0.3655,  ..., -1.4812, -0.9510, -0.5386]],\n",
       "\n",
       "         [[-0.2628,    -inf,    -inf,  ...,    -inf,    -inf,    -inf],\n",
       "          [-0.3414, -0.6880,    -inf,  ...,    -inf,    -inf,    -inf],\n",
       "          [-0.1750,  0.5695,  0.3346,  ...,    -inf,    -inf,    -inf],\n",
       "          ...,\n",
       "          [ 0.2182, -0.6491, -0.5636,  ...,  0.2614,    -inf,    -inf],\n",
       "          [ 0.1642, -0.7648, -0.3559,  ...,  0.3475, -1.0672,    -inf],\n",
       "          [-0.0468, -0.1908, -0.0741,  ...,  0.2862, -0.3812,  0.1057]],\n",
       "\n",
       "         [[ 0.1853,    -inf,    -inf,  ...,    -inf,    -inf,    -inf],\n",
       "          [ 0.5593, -0.1041,    -inf,  ...,    -inf,    -inf,    -inf],\n",
       "          [-0.2747,  0.2198, -0.1199,  ...,    -inf,    -inf,    -inf],\n",
       "          ...,\n",
       "          [-0.3247, -0.4508,  0.3206,  ...,  0.5579,    -inf,    -inf],\n",
       "          [ 0.1676,  0.5925, -0.1506,  ..., -0.4180, -0.3813,    -inf],\n",
       "          [ 0.1140,  0.1061,  0.1683,  ...,  0.3958,  0.3248,  0.2202]],\n",
       "\n",
       "         [[ 0.2787,    -inf,    -inf,  ...,    -inf,    -inf,    -inf],\n",
       "          [ 0.0211,  0.2684,    -inf,  ...,    -inf,    -inf,    -inf],\n",
       "          [ 0.0077,  0.3771, -0.4235,  ...,    -inf,    -inf,    -inf],\n",
       "          ...,\n",
       "          [ 0.2477, -0.1104, -0.1401,  ..., -0.7752,    -inf,    -inf],\n",
       "          [ 0.0754, -0.2425, -0.5073,  ..., -0.7553, -0.0951,    -inf],\n",
       "          [ 0.0298,  0.0758, -0.7539,  ..., -0.8680, -0.8661, -0.8100]]]],\n",
       "       grad_fn=<MaskedFillBackward0>)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# apply the mask，矩阵上三角（不包括对角线取-inf)\n",
    "mask = torch.triu(torch.ones(token_length, token_length), diagonal=1).bool()\n",
    "output = output.masked_fill(mask, float('-inf'))\n",
    "\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[1.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.3912, 0.6088, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.3471, 0.2913, 0.3616,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          ...,\n",
       "          [0.0859, 0.0604, 0.0683,  ..., 0.0482, 0.0000, 0.0000],\n",
       "          [0.0316, 0.0633, 0.0637,  ..., 0.1810, 0.0344, 0.0000],\n",
       "          [0.0503, 0.1106, 0.0305,  ..., 0.1012, 0.1099, 0.0511]],\n",
       "\n",
       "         [[1.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.8050, 0.1950, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.4101, 0.2685, 0.3214,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          ...,\n",
       "          [0.0514, 0.0304, 0.0696,  ..., 0.0557, 0.0000, 0.0000],\n",
       "          [0.0275, 0.0552, 0.0242,  ..., 0.0427, 0.0284, 0.0000],\n",
       "          [0.0479, 0.0362, 0.1002,  ..., 0.0674, 0.0349, 0.0589]],\n",
       "\n",
       "         [[1.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.2940, 0.7060, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.3475, 0.1931, 0.4594,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          ...,\n",
       "          [0.0534, 0.0437, 0.0770,  ..., 0.1119, 0.0000, 0.0000],\n",
       "          [0.0623, 0.0630, 0.0716,  ..., 0.0529, 0.0352, 0.0000],\n",
       "          [0.0518, 0.0652, 0.0876,  ..., 0.0538, 0.0567, 0.0676]],\n",
       "\n",
       "         [[1.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.4423, 0.5577, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.1724, 0.2632, 0.5644,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          ...,\n",
       "          [0.1160, 0.0564, 0.0387,  ..., 0.0659, 0.0000, 0.0000],\n",
       "          [0.0960, 0.0682, 0.0634,  ..., 0.0551, 0.0417, 0.0000],\n",
       "          [0.0644, 0.1270, 0.0604,  ..., 0.0683, 0.0951, 0.0489]]],\n",
       "\n",
       "\n",
       "        [[[1.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.4762, 0.5238, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.3554, 0.2390, 0.4056,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          ...,\n",
       "          [0.0954, 0.0798, 0.0351,  ..., 0.0731, 0.0000, 0.0000],\n",
       "          [0.0576, 0.1161, 0.0564,  ..., 0.0401, 0.0708, 0.0000],\n",
       "          [0.0719, 0.1087, 0.0668,  ..., 0.0646, 0.0602, 0.0527]],\n",
       "\n",
       "         [[1.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.2949, 0.7051, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.3137, 0.4475, 0.2388,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          ...,\n",
       "          [0.1146, 0.0435, 0.0785,  ..., 0.0627, 0.0000, 0.0000],\n",
       "          [0.0665, 0.0631, 0.0633,  ..., 0.0789, 0.0750, 0.0000],\n",
       "          [0.0355, 0.0534, 0.0635,  ..., 0.1037, 0.0684, 0.0463]],\n",
       "\n",
       "         [[1.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.6350, 0.3650, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.2967, 0.3388, 0.3645,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          ...,\n",
       "          [0.0733, 0.1278, 0.0704,  ..., 0.0681, 0.0000, 0.0000],\n",
       "          [0.0288, 0.1547, 0.0702,  ..., 0.0344, 0.0618, 0.0000],\n",
       "          [0.0701, 0.1124, 0.1069,  ..., 0.0573, 0.0416, 0.0657]],\n",
       "\n",
       "         [[1.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.6305, 0.3695, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.1579, 0.3551, 0.4870,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          ...,\n",
       "          [0.0456, 0.0741, 0.1536,  ..., 0.0542, 0.0000, 0.0000],\n",
       "          [0.0806, 0.0644, 0.0677,  ..., 0.0573, 0.0671, 0.0000],\n",
       "          [0.0843, 0.0449, 0.0674,  ..., 0.0609, 0.0558, 0.0857]]],\n",
       "\n",
       "\n",
       "        [[[1.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.5077, 0.4923, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.3456, 0.3222, 0.3322,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          ...,\n",
       "          [0.0666, 0.0736, 0.0775,  ..., 0.0461, 0.0000, 0.0000],\n",
       "          [0.0633, 0.0842, 0.0560,  ..., 0.0307, 0.2103, 0.0000],\n",
       "          [0.0750, 0.0562, 0.0773,  ..., 0.0519, 0.0303, 0.0615]],\n",
       "\n",
       "         [[1.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.7125, 0.2875, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.3931, 0.2640, 0.3429,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          ...,\n",
       "          [0.0655, 0.0605, 0.0383,  ..., 0.0473, 0.0000, 0.0000],\n",
       "          [0.1060, 0.0393, 0.0286,  ..., 0.0640, 0.0656, 0.0000],\n",
       "          [0.0667, 0.0364, 0.0483,  ..., 0.1111, 0.0834, 0.0363]],\n",
       "\n",
       "         [[1.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.4250, 0.5750, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.3859, 0.4165, 0.1976,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          ...,\n",
       "          [0.0451, 0.0781, 0.0453,  ..., 0.0663, 0.0000, 0.0000],\n",
       "          [0.0335, 0.1500, 0.0670,  ..., 0.1066, 0.1248, 0.0000],\n",
       "          [0.0362, 0.1130, 0.0518,  ..., 0.0400, 0.1453, 0.0328]],\n",
       "\n",
       "         [[1.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.5031, 0.4969, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.4048, 0.1651, 0.4301,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          ...,\n",
       "          [0.0705, 0.1213, 0.0876,  ..., 0.0432, 0.0000, 0.0000],\n",
       "          [0.0607, 0.0478, 0.0997,  ..., 0.1225, 0.0896, 0.0000],\n",
       "          [0.0931, 0.0503, 0.0537,  ..., 0.0658, 0.0419, 0.0353]]],\n",
       "\n",
       "\n",
       "        [[[1.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.6009, 0.3991, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.3555, 0.2375, 0.4070,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          ...,\n",
       "          [0.0970, 0.0548, 0.0797,  ..., 0.0510, 0.0000, 0.0000],\n",
       "          [0.0634, 0.0310, 0.1073,  ..., 0.0404, 0.0229, 0.0000],\n",
       "          [0.0857, 0.0965, 0.0593,  ..., 0.0194, 0.0330, 0.0499]],\n",
       "\n",
       "         [[1.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.5858, 0.4142, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.2096, 0.4414, 0.3490,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          ...,\n",
       "          [0.0791, 0.0332, 0.0362,  ..., 0.0826, 0.0000, 0.0000],\n",
       "          [0.0629, 0.0248, 0.0374,  ..., 0.0755, 0.0184, 0.0000],\n",
       "          [0.0593, 0.0514, 0.0577,  ..., 0.0828, 0.0425, 0.0691]],\n",
       "\n",
       "         [[1.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.6600, 0.3400, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.2627, 0.4307, 0.3066,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          ...,\n",
       "          [0.0455, 0.0401, 0.0867,  ..., 0.1099, 0.0000, 0.0000],\n",
       "          [0.0714, 0.1093, 0.0520,  ..., 0.0398, 0.0413, 0.0000],\n",
       "          [0.0622, 0.0618, 0.0657,  ..., 0.0825, 0.0769, 0.0692]],\n",
       "\n",
       "         [[1.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.4385, 0.5615, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.3229, 0.4672, 0.2098,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          ...,\n",
       "          [0.0891, 0.0623, 0.0605,  ..., 0.0320, 0.0000, 0.0000],\n",
       "          [0.0737, 0.0536, 0.0411,  ..., 0.0321, 0.0621, 0.0000],\n",
       "          [0.0594, 0.0622, 0.0271,  ..., 0.0242, 0.0242, 0.0256]]]],\n",
       "       grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# softmax, dim = -1对每一行进行softmax\n",
    "attention_score = torch.softmax(output, dim=-1)\n",
    "attention_score"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
