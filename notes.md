## Tokenization
使用Python的tiktoken库，编码方式为"cl100k_base"。为什么选择这个？GPT4也用的这个。对原文本进行Encoding。

text -> token
## Word Embedding
在这里，我们将有一个batch_x与batch_y，batch_y实际上只是x往右偏移了一个。这错位的一个就是要将x的预测值与y的实际值对比。也就是x是训练集，y是预测集。
```py
x_batch = torch.stack([data[idx:idx + context_length] for idx in idxs])
y_batch = torch.stack([data[idx + 1:idx + context_length + 1] for idx in idxs])
```

word embedding是文本表示的一类方法。这里选用word2vec。
对所有的token进行nn.embedding，扩展到d_model这么多维度。也就是一个单词是1 * d_model的向量。

token -> 1 * d_model vector

## Postional Embedding
和transformer一样，在这里我们用sine和cosine来生成PE。

最后对于`总的Embedding`，只需将Word Embedding与PE`直接相加`就行。

## Transformer Block
现在我们有input Embedding X。开始多头注意力机制。

### QKV
Query: 查询，要查询的信息
Key：索引，被查询的向量
Value：内容，查询得到的值

Q * K也就是查询Q与K的相似度，这个相似度就是注意力的权重alpha。将alpha与Value相乘，也就是我们需要以多大的注意力观察Value。

QKV都是从特征本身得到的，实际上都是将输入乘以Wq, Wk, Wv三个通过学习得到的矩阵得到的新的矩阵。这三个Wq的初始值都是随机的。

Q = X*Wq

K = X*Wk

V = X*Wv

为什么需要Q * K^T / sqrt(dk)，这个除以根号dk的操作是为了防止Q与K的内积过大，使梯度稳定的方法。

此时只算出来Q * K^T / sqrt(dk)这个值，并没有取Softmax。

为了实现多头注意力机制，我们要将d_model拆成多头的数量。假设我们要4个多头，也就是每个头有d_model / 4个维度。

### Mask
取矩阵下三角。对角线以及以上都是inf。此时才进行Softmax。

得到attention_score这个矩阵。


```
tensor([[[[1.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
          [0.3869, 0.6131, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
          [0.2592, 0.4375, 0.3033,  ..., 0.0000, 0.0000, 0.0000],
          ...,
          [0.0614, 0.0457, 0.0789,  ..., 0.0638, 0.0000, 0.0000],
          [0.0487, 0.0595, 0.0684,  ..., 0.0396, 0.0569, 0.0000],
          [0.0282, 0.0759, 0.0825,  ..., 0.0267, 0.0471, 0.0484]],

         [[1.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
          [0.5713, 0.4287, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
          [0.5335, 0.2957, 0.1708,  ..., 0.0000, 0.0000, 0.0000],
          ...,
          [0.0520, 0.0511, 0.0500,  ..., 0.0664, 0.0000, 0.0000],
          [0.0810, 0.0983, 0.0618,  ..., 0.0827, 0.0725, 0.0000],
          [0.0543, 0.0635, 0.0346,  ..., 0.0826, 0.0552, 0.0705]],

         [[1.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
          [0.4454, 0.5546, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
          [0.3290, 0.2731, 0.3979,  ..., 0.0000, 0.0000, 0.0000],
          ...,
          [0.0666, 0.0811, 0.0557,  ..., 0.0475, 0.0000, 0.0000],
          [0.0658, 0.0571, 0.0824,  ..., 0.0265, 0.0571, 0.0000],
          [0.0680, 0.0428, 0.0699,  ..., 0.0552, 0.0596, 0.1035]],

         [[1.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
...
          ...,
          [0.0524, 0.0648, 0.0686,  ..., 0.0522, 0.0000, 0.0000],
          [0.0390, 0.0555, 0.0926,  ..., 0.0340, 0.0537, 0.0000],
          [0.0530, 0.0565, 0.0358,  ..., 0.0845, 0.0655, 0.0308]]]],
       grad_fn=<SoftmaxBackward0>)
```

### Calculate V Attention
最后将attention_score矩阵与V相乘。

### Concatenate
这一步将所有的Head的值都拼接起来成为一个Head，并将他们扔到一个线形层中。

拼接后得到的矩阵为 A。维度是d_model。同样的，为了得到输出，也需要将A乘以一个系数矩阵Wo。