# Loss函数优化

## 梯度下降法
最传统的权重更新算法。
1. 基本思想：
    先设定一个学习率$\eta$，参数沿梯度的反方向移动。假设需要更新的参数为$w$，$w$，梯度为$g$，则其更新策略可以为:
    $$ w \leftarrow w - \eta * g $$
2. 梯度下降法三种：
   1. BGD(Batch Gradient Descent): 批量梯度下降，每次参数更新使用所有样本
   2. SGD(Stochastic Gradient Descent)：随机梯度下降，每次参数更新只使用一个样本
   3. MBGD(Mini-Batch Gradient Descent)：小批量梯度下降，每次参数更新使用小部分数据样本(mini_batch)
   + 这三种参数优化过程都类似：
    1. $g = \frac{\partial loss} {\partial w}$
    2. 求梯度的均值
    3. 更新权重: $w \leftarrow w - \eta * g$
3. 优缺点：
   1. 算法简洁，当学习率取值恰当时，可以收敛到局部最优或者全局最优点
   2. 对超参数学习率敏感
   3. 容易被卡在鞍点
   4. 在较平坦的区域，由于梯度接近于0，优化算法会误判导致提前结束迭代陷入局部极小值
4. 多维梯度下降法：
   多元损失函数，它的梯度也是多元的，是由d个偏导数组成的向量：
$$
\nabla f(X)=\left[\frac{\partial f_x}{\partial x_1}, \frac{\partial f_x}{\partial x_2}, \cdots, \frac{\partial f_x}{\partial x_d}\right]^T
$$
然后选择合适的学率进行梯度下降：
$$
x_i \leftarrow x_i-\eta * \nabla f(X)
$$

## 动量(Momentum)
为了让参数的更新具有惯性，每一步更新都是由前面梯度的累积$v$和当前点梯度$g$组成的。
+ 累计梯度更新：$v \leftarrow \alpha v + (1 - \alpha)g$，其中$\alpha$为动量参数，$v$为累计梯度，$g$为当前梯度，$\eta$为学习率。这种其实也叫加权平均值。
+ 加权平均值存在一个问题，$v$这个累积梯度初始值是0，导致最开始的几组值会比真实值小。后续可以采取措施比如乘以$\frac{1}{1 - \beta ^ t}$。
+ 梯度更新：$x \leftarrow x - \eta * v$

优点：
1. 加快收敛能帮助参数在正确方向上加速前进
2. 它可以帮助跳出局部最小值

### Adagrad
自适应学习率优化算法。之前的随机梯度下降，对所有的参数都采用相同的固定的学习率进行优化。当损失函数参数比较多，系数量级差的比较大时，使用相同的学习率，效果可能不会很好。

举例，假设损失函数是$f(x)=x_1^2+10 x_2^2$，x和y的初值分别是$x_1 = 40$，$x_2 = 20$。通过观察，我们可以知道$x_1 = 0, x_2 = 0$是两个参数的极值点。下一步的梯度值为$\frac{\partial \text { loss }}{\partial x_1}=80, \quad \frac{\partial \text { loss }}{\partial x_2}=400$，可以知道，**$x_1$要移动的步幅远小于$x_2$**。此时，如果用相同的学习率，那么效果肯定会不好，因为无法保证对所有的系数都达到合理的值。